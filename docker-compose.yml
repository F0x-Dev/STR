version: "3.8"

services:

  nginx-rtmp:
    build: ./nginx-rtmp
    container_name: nginx-rtmp
    restart: always
    volumes:
      - ./data/hls:/var/www/hls
      - ./nginx/conf/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf/rtmp.conf:/etc/nginx/rtmp.conf:ro
    ports:
      - "1935:1935"
      - "8080:80"
    environment:
      - HLS_PATH=/var/www/hls
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 256M
    networks:
      - frontend
      - backend

  ffmpeg-worker:
    image: jrottenberg/ffmpeg:5.1-alpine
    container_name: ffmpeg-worker
    restart: on-failure
    depends_on:
      nginx-rtmp:
        condition: service_healthy
    volumes:
      - ./data/hls:/var/www/hls
      - ./ffmpeg/scripts:/app/scripts:ro
      - /dev/shm:/dev/shm
    # Keep container running - processes are spawned by python-app
    entrypoint: ["/bin/sh", "-c", "while true; do sleep 3600; done"]
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 512M
    networks:
      - backend

  python-app:
    build: ./python-app
    container_name: python-orchestrator
    restart: always
    ports:
      - "8000:8000"
    env_file:
      - .env
    environment:
      - JWT_SECRET=${JWT_SECRET}
      - ADMIN_USER=${ADMIN_USER:-admin}
      - ADMIN_PASSWORD=${ADMIN_PASSWORD}
      - DB_PATH=/app/data/database.db
      - HLS_PATH=/var/www/hls
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - RATE_LIMIT_PER_MINUTE=${RATE_LIMIT_PER_MINUTE:-60}
      - MAX_CONCURRENT_STREAMS=${MAX_CONCURRENT_STREAMS:-10}
    volumes:
      - ./ffmpeg/scripts:/app/scripts:ro
      - ./data/hls:/var/www/hls:rw
      - ./data:/app/data:rw
    depends_on:
      ffmpeg-worker:
        condition: service_started
      nginx-rtmp:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 256M
    networks:
      - frontend
      - backend
      - monitoring

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 128M
    networks:
      - monitoring

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-storage:/var/lib/grafana
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 128M
    networks:
      - monitoring
      - frontend

  proxy:
    image: nginx:stable-alpine
    container_name: nginx-proxy
    restart: always
    volumes:
      - ./proxy/conf.d:/etc/nginx/conf.d:ro
      - ./data/hls:/var/www/hls:ro
      - ./certs:/etc/letsencrypt:ro
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      nginx-rtmp:
        condition: service_healthy
      python-app:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 64M
    networks:
      - frontend

networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge
    internal: true
  monitoring:
    driver: bridge

volumes:
  grafana-storage:
    driver: local
  prometheus-data:
    driver: local
